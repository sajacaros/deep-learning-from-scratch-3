# STEP 28 함수 최적화
## 28.1 로젠브록 함수
* 로젠브록 함수
  * Rosenbrock function
  * ![로젠브록](../../images/식%2028.1.png)
  * 바나나를 닮아(?) 바나나 함수라고도 불림
  * ![로젠브록](../../images/그림%2028-1.png)
* 이번장의 목표
  * 로젠브록 함수의 출력이 최소가 되는 x0,x1을 찾는것
    * 참고) 최솟값(x0,x1) = (1,1)
## 28.2 미분 계산하기
* 로젠브록 함수 (0.0, 2.0)에서의 미분
* 로젠브록 함수 구현
``` 
import numpy as np
from dezero import Variable

def rosenbrock(x0, x1):
    y = 100 * (x1 - x0 ** 2) ** 2 + (1 - x0) ** 2
    return y
```
* 사용예
``` 
x0 = Variable(np.array(0.0))
x1 = Variable(np.array(2.0))

y = rosenbrock(x0, x1)
y.backward()
print(x0.grad, x1.grad) # -2.0, 400.0
```
* 미분값 (-2.0, 400.0)을 기울기(gradient), 기울기 벡터라 부름
  * (x0, x1)=(0.0, 2.0) 지점에서 y값을 가장 크게 늘려주는 방향이 (-2.0, 400)임을 의미
  * 기울기 벡터에 마이너스를 곱해주면 y값을 가장 작게 줄여주는 방향을 뜻함
## 28.3 경사하강법 구현
* 경사하강법
  * gradient descent 
  * 기울기 방향으로 일정 거리만큼 이동하여 다시 기울기를 구하는 작업을 반복하면 최댓값 혹은 최소값에 접근
* 로젠브록 함수의 기울기 방향에 마이너스를 곱한 방향으로 이동하는 코드 구현
``` 
x0 = Variable(np.array(0.0))
x1 = Variable(np.array(2.0))
lr = 0.001 # 학습률
iters = 1000 # 반복 횟수

for i in range(iters):
    print(x0, x1)
    y = rosenbrock(x0, x1)
    x0.cleargrad()
    x1.cleargrad()
    y.backward()
    
    x0.data -= lr * x0.grad
    x1.data -= lr * x1.grad
```
* 갱신 경로(lr=0.001, iters=1000)
  * 별이 최솟값의 위치
  * (x0, x1) = (0.683..., 0.465...)
  * ![갱신 경로](../../images/그림%2028-2.png)
* iters를 10000으로 늘려 다시 실행
  * (x0, x1) = (0.994..., 0.989...) 
  * ![갱신 경로](../../images/그림%2028-3.png)
* 결론
  * 경사하강법으로 최소값을 찾음
  * iters를 50000으로 늘리면 (1.0, 1.0) 위치에 겨우 도착함
    * 10000회에도 충분히 접근했으므로 50000회는 너무 과함
  * 경사하강법은 실제로 로젠브록 같이 골짜기가 길게 뻗은 함수에 잘 대응 못함
    * 다른 최적화 방법이 있음
